{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pZwJTrKJmmsc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import correlate2d\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input Data**"
      ],
      "metadata": {
        "id": "pO_Ds2dBSB75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train = train_images[:5000] / 255.0\n",
        "y_train = train_labels[:5000]\n",
        "\n",
        "X_test = train_images[5000:10000] / 255.0\n",
        "y_test = train_labels[5000:10000]\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DyQoABNvDuK",
        "outputId": "d0dff423-3bad-410e-84bc-55b8595f93b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59aPjB3ZvRPE",
        "outputId": "261bc2bd-e812-4f05-a5c2-70fa1669c131"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolution Layer**"
      ],
      "metadata": {
        "id": "jbLjNUdMR5Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "  def __init__(self, input_shape, filter_size, num_filters):\n",
        "    input_height, input_width = input_shape\n",
        "    self.num_filters = num_filters\n",
        "    self.input_shape = input_shape\n",
        "\n",
        "    #Size of outputs and filters\n",
        "    self.filter_shape = (num_filters, filter_size, filter_size) #(3,3)\n",
        "    self.output_shape = (num_filters, input_height - filter_size + 1, input_width - filter_size + 1)\n",
        "\n",
        "    #Initialize the convolutional filters and biases with random values\n",
        "    self.filters = np.random.randn(*self.filter_shape) #shape is determined by filter shape\n",
        "    self.biases = np.random.randn(*self.output_shape) #shape is determined by output shape\n",
        "\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    #Storing the input data for later use\n",
        "    self.input_data = input_data\n",
        "\n",
        "    #shape of this 0 filled output array is determined by the output shape\n",
        "    output = np.zeros(self.output_shape)\n",
        "\n",
        "    #Performing Convolution between the input data and the current filter (convolution operation for each filter)\n",
        "    for i in range(self.num_filters):\n",
        "      output[i] = correlate2d(self.input_data, self.filters[i], mode=\"valid\")\n",
        "\n",
        "    #Applying Relu Activtion function(element wise). This sets all -ve values to 0 and introduces non-linearity to our model\n",
        "    output = np.maximum(output, 0)\n",
        "    return output\n",
        "\n",
        "\n",
        "  def backward(self, dL_dout, lr):\n",
        "    #Initialize arrays to store the gradients of the loss w.r.t the input data and the filters\n",
        "    dL_dinput = np.zeros_like(self.input_data)\n",
        "    dL_dfilters = np.zeros_like(self.filters)\n",
        "\n",
        "    #Calculate gradient of the loss w.r.t the filters(for updating the filters during optimization filters)\n",
        "    for i in range(self.num_filters):\n",
        "      dL_dfilters[i] = correlate2d(self.input_data, dL_dout[i],mode=\"valid\")\n",
        "\n",
        "      #Calculating the gradient of loss w.r.t inputs\n",
        "      dL_dinput += correlate2d(dL_dout[i],self.filters[i], mode=\"full\")\n",
        "\n",
        "    #Updating the filters and bias with learning rate and the gradients\n",
        "    self.filters -= lr * dL_dfilters\n",
        "    self.biases -= lr * dL_dout\n",
        "\n",
        "    #returning the gradient of inputs\n",
        "    return dL_dinput"
      ],
      "metadata": {
        "id": "4Agb_XYHtCTt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Max-Pooling Layer**"
      ],
      "metadata": {
        "id": "6UmyfArISFTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool:\n",
        "  def __init__(self, pool_size):\n",
        "    self.pool_size = pool_size\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    self.input_data = input_data\n",
        "    self.num_channels, self.input_height, self.input_width = input_data.shape\n",
        "    self.output_height = self.input_height // self.pool_size\n",
        "    self.output_width = self.input_width // self.pool_size\n",
        "\n",
        "    #An array filled with 0 to store the output of Max-Pooling function. Shape is determined by the 3 parameters\n",
        "    self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
        "\n",
        "    #Iterating over each channels of input_data\n",
        "    for c in range(self.num_channels):\n",
        "\n",
        "      #Iterate through the height of output feature map\n",
        "      for i in range(self.output_height):\n",
        "\n",
        "        #Iterate through the width of output feature map\n",
        "        for j in range(self.output_width):\n",
        "\n",
        "          #Calculates the starting and ending positions of the current patch in the input data\n",
        "          start_i = i * self.pool_size\n",
        "          start_j = j * self.pool_size\n",
        "          end_i = start_i + self.pool_size\n",
        "          end_j = start_j + self.pool_size\n",
        "\n",
        "          #Extracts a patch from the input data based on the calculated coordinates\n",
        "          patch = input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "          #perform the max-pooling operation for the current patch and Finding the maximum value from each patch/window\n",
        "          self.output[c, i, j] = np.max(patch)\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def backward(self, dL_dout, lr):\n",
        "    #Initialize an array to store the gradient of the loss w.r.t the input data.\n",
        "    dL_dinput = np.zeros_like(self.input_data)\n",
        "\n",
        "    for c in range(self.num_channels):\n",
        "      for i in range(self.output_height):\n",
        "        for j in range(self.output_width):\n",
        "          start_i = i * self.pool_size\n",
        "          start_j = j * self.pool_size\n",
        "          end_i = start_i + self.pool_size\n",
        "          end_j = start_j + self.pool_size\n",
        "\n",
        "          patch = self.input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "          #Creates a binary mask where the maximum value in the patch is marked as True\n",
        "          mask = patch == np.max(patch)\n",
        "\n",
        "          #Assigns the gradient of the loss w.r.t the output multiplied by the binary mask to the corresponding positions in the gradient array\n",
        "          dL_dinput[c,start_i:end_i, start_j:end_j] = dL_dout[c, i, j] * mask\n",
        "\n",
        "    return dL_dinput"
      ],
      "metadata": {
        "id": "6QxhCl3ntkFc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dense(Softmax) Layer**"
      ],
      "metadata": {
        "id": "ypJgxyxnSI3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Fully_Connected:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    #Initialize random values of weights and biases\n",
        "    self.weights = np.random.randn(output_size, self.input_size)\n",
        "    self.biases = np.random.rand(output_size, 1)\n",
        "\n",
        "  def softmax(self, z):\n",
        "    #Shift the input values to avoid numerical instability to prevent large exponentiated values\n",
        "    shifted_z = z - np.max(z)\n",
        "    exp_values = np.exp(shifted_z)\n",
        "\n",
        "    #Sum used in the softmax denominator\n",
        "    sum_exp_values = np.sum(exp_values, axis=0)\n",
        "\n",
        "    #Log useful for numerical stability\n",
        "    log_sum_exp = np.log(sum_exp_values)\n",
        "\n",
        "    #Calculate the softmax probabilities(resulting probabilities sum to 1)\n",
        "    probabilities = exp_values / sum_exp_values\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "  #calculate the derivative of the softmax activation function\n",
        "  def softmax_derivative(self, s):\n",
        "\n",
        "    #Create a diagonal matrix with the elements of the input vector s and subtract the outer product of the softmax probabilities s\n",
        "    return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    self.input_data = input_data\n",
        "\n",
        "    #Flattens/converts the input data into a 1D array(vector)\n",
        "    flattened_input = input_data.flatten().reshape(1, -1)\n",
        "\n",
        "    #Computing the linear transformation\n",
        "    self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
        "\n",
        "    #Applying Softmax to to the abouve output(sums to 1)\n",
        "    self.output = self.softmax(self.z)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dL_dout, lr):\n",
        "\n",
        "    #Calculate the gradient of the loss using chain rule w.r.t the pre-activation z\n",
        "    dL_dy = np.dot(self.softmax_derivative(self.output), dL_dout)\n",
        "\n",
        "    #Calculate the gradient of the loss w.r.t weights\n",
        "    dL_dw = np.dot(dL_dy, self.input_data.flatten().reshape(1, -1))\n",
        "\n",
        "    #Calculate the gradient of the loss w.r.t biases\n",
        "    dL_db = dL_dy\n",
        "\n",
        "    #Calculate the gradient of the loss w.r.t input data\n",
        "    dL_dinput = np.dot(self.weights.T, dL_dy)\n",
        "    dL_dinput = dL_dinput.reshape(self.input_data.shape)\n",
        "\n",
        "    #Update the weights and biases based on the learning rate and gradients\n",
        "    self.weights -= lr * dL_dw\n",
        "    self.biases -= lr * dL_db\n",
        "\n",
        "    #Return the gradient of the loss w.r.t the input data\n",
        "    return dL_dinput\n"
      ],
      "metadata": {
        "id": "eCBBtYtuuPgl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cross-Entropy Loss**"
      ],
      "metadata": {
        "id": "l-uqJ4RJSOY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(predictions, targets):\n",
        "  num_samples = 10\n",
        "\n",
        "  #Using epsilon value to avoid numerical instability\n",
        "  epsilon = 1e-7\n",
        "  predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "\n",
        "  #Calcultae cross entropy loss\n",
        "  loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
        "  return loss\n",
        "\n",
        "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
        "  num_samples = actual_labels.shape[0]\n",
        "  gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
        "\n",
        "  return gradient\n"
      ],
      "metadata": {
        "id": "CDp75gUuup-o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ],
      "metadata": {
        "id": "jTjlzhSFSSI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize a convolutional layer with the specified input shape, number of filters, and filter size\n",
        "conv = Convolution(X_train[0].shape, 6, 1)\n",
        "\n",
        "#initialize a max-pooling layer with a pooling size of 2\n",
        "pool = MaxPool(2)\n",
        "\n",
        "#initialize a fully connected layer with the specified input and output size (10 no of neurons)\n",
        "full = Fully_Connected(121, 10)\n",
        "\n",
        "def train_network(X, y, conv, pool, full, lr=0.01, epochs=50):\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(X)):\n",
        "\n",
        "      #Each input sample X[i] is passed through the convolutional layer, the max-pooling layer, and the fully connected layer\n",
        "      conv_out = conv.forward(X[i])\n",
        "      pool_out = pool.forward(conv_out)\n",
        "      full_out = full.forward(pool_out)\n",
        "\n",
        "      #Calculate the cross entropy loss\n",
        "      loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
        "      total_loss += loss\n",
        "\n",
        "      #Converting to One-Hot encoding to compare with the ground truth values\n",
        "      one_hot_pred = np.zeros_like(full_out)\n",
        "\n",
        "      #find the max value in the output layer and set it to 1\n",
        "      one_hot_pred[np.argmax(full_out)] = 1\n",
        "\n",
        "      #Flatten to match the output shape of the ground truth labels\n",
        "      one_hot_pred = one_hot_pred.flatten()\n",
        "\n",
        "      #Find the index of the max value in predictions and ground truth labels\n",
        "      num_pred = np.argmax(one_hot_pred)\n",
        "      num_y = np.argmax(y[i])\n",
        "\n",
        "      if num_pred == num_y:\n",
        "        correct_predictions += 1\n",
        "\n",
        "      #perform backward propagation\n",
        "      gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
        "      full_back = full.backward(gradient, lr)\n",
        "      pool_back = pool.backward(full_back, lr)\n",
        "      conv_back = conv.backward(pool_back, lr)\n",
        "\n",
        "    #Printing the epoch statistics\n",
        "    average_loss = total_loss / len(X)\n",
        "    accuracy = correct_predictions / len(X_train) * 100.0\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "2UL08RO5uqrs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predictions**"
      ],
      "metadata": {
        "id": "Zs6oVZyhSVhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_sample, conv, pool, full):\n",
        "\n",
        "  #Perform forward pass through Convolution and pooling\n",
        "  conv_out = conv.forward(input_sample)\n",
        "  pool_out = pool.forward(conv_out)\n",
        "\n",
        "  #Flattening the layer to match the output shape\n",
        "  flattened_output = pool_out.flatten()\n",
        "\n",
        "  #Perform forward pass through fully connected layer\n",
        "  predictions = full.forward(flattened_output)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "tWBqAFccuvsT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_network(X_train, y_train, conv, pool, full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YY-W_ZOvTom",
        "outputId": "1b5999b2-ae84-4f6c-eb81-175ffefae7c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 1.0678 - Accuracy: 22.50%\n",
            "Epoch 2/20 - Loss: 0.7336 - Accuracy: 31.74%\n",
            "Epoch 3/20 - Loss: 0.3990 - Accuracy: 28.56%\n",
            "Epoch 4/20 - Loss: 0.2169 - Accuracy: 36.08%\n",
            "Epoch 5/20 - Loss: 0.1669 - Accuracy: 45.72%\n",
            "Epoch 6/20 - Loss: 0.1445 - Accuracy: 51.70%\n",
            "Epoch 7/20 - Loss: 0.1282 - Accuracy: 56.16%\n",
            "Epoch 8/20 - Loss: 0.1173 - Accuracy: 59.12%\n",
            "Epoch 9/20 - Loss: 0.1102 - Accuracy: 61.62%\n",
            "Epoch 10/20 - Loss: 0.1050 - Accuracy: 63.24%\n",
            "Epoch 11/20 - Loss: 0.1011 - Accuracy: 64.52%\n",
            "Epoch 12/20 - Loss: 0.0979 - Accuracy: 65.68%\n",
            "Epoch 13/20 - Loss: 0.0952 - Accuracy: 66.50%\n",
            "Epoch 14/20 - Loss: 0.0930 - Accuracy: 67.52%\n",
            "Epoch 15/20 - Loss: 0.0910 - Accuracy: 68.54%\n",
            "Epoch 16/20 - Loss: 0.0892 - Accuracy: 69.26%\n",
            "Epoch 17/20 - Loss: 0.0876 - Accuracy: 69.60%\n",
            "Epoch 18/20 - Loss: 0.0861 - Accuracy: 70.22%\n",
            "Epoch 19/20 - Loss: 0.0849 - Accuracy: 70.46%\n",
            "Epoch 20/20 - Loss: 0.0837 - Accuracy: 71.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for data in X_test:\n",
        "  pred = predict(data, conv, pool, full)\n",
        "  one_hot_pred = np.zeros_like(pred)\n",
        "  one_hot_pred[np.argmax(pred)] = 1\n",
        "  predictions.append(one_hot_pred.flatten())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "r7BHhnIJvX9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca271a12-5127-444b-cd63-164405856996"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(predictions, y_test)"
      ],
      "metadata": {
        "id": "Sc8sVuIhxP7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891642b4-ed28-49f2-a80a-3c2798c93d01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6818"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}